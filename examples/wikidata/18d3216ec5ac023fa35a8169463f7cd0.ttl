@prefix sh: <http://www.w3.org/ns/shacl#> .
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix dcterms: <http://purl.org/dc/terms/> .
@prefix schema: <https://schema.org/> .

<https://www.wikidata.org/#query-18d3216ec5ac023fa35a8169463f7cd0> a sh:SPARQLExecutable,
    sh:SPARQLSelectExecutable;
  sh:select """SELECT ?item ?itemLabel ?itemDescription ?v ?place ?placeLabel
{
    ?item p:P131 [ pq:P6375 ?v ; ps:P131 ?place ] .
    ?place wdt:P17 wd:Q142 . 
    SERVICE wikibase:label { bd:serviceParam wikibase:language \"fr,en\" .}          
}""";
  rdfs:comment """This is an archive of past discussions. Do not edit the contents of this page. If you wish to start a new discussion or revive an old one, please do so on the current talk page. Contents 1 Semi-automated import of information from Commons categories containing a \"Category definition: Object\" template 2 Upload triples to Wikidata 3 OpenCitations bot 4 Magyarország közigazgatási helynévkönyve, 2018. január 1. (hungarian) 5 \"Street in\" description 6 Disambiguate \"Siracusa\" football clubs 7 Mark deprecated P1255 if the value starts with vlts 8 ORCID for Wikidata 3 9 Administrative division code of Chongqing Municipal District, People's Republic of China 10 cleanup of English descriptions for people 11 Translation of the Indian Women Entreprenuer in Gujarati 12 Complete Google Books ID (P675) from Internet Archive ID (P724) 13 Upgrade URLs to HTTPS, whenever possible 14 Getting number of cases (P1603) to rank most current figure as preferred 15 Permissions for new qualifiers \"latest start date\" and \"earliest end date\" 16 Research institutes in France 17 weekly import of new articles (periodic data import) 18 Modules and template: sync with from source Semi-automated import of information from Commons categories containing a \"Category definition: Object\" template Request date: 5 February 2018, by: Rama Link to discussions justifying the request  Task description Commons categories about one specific object (such as a work of art, archaeological item, etc.) can be described with a \"Category definition: Object\" template [1]. This information is essentially a duplicate of what is or should be on Wikidata.To prove this point, I have drafted a \"User:Rama/Catdef\" template that uses Lua to import all relevant information from Wikidata and reproduces all the features of \"Category definition: Object\", while requiring only the Q-Number as parameter (see Category:The_Seated_Scribe for instance). This template has the advantage of requesting Wikidata labels to render the information, and is thus much more multi-lingual than the hand-labeled version (try fr, de, ja, etc.).I am now proposing to deploy another script to do the same thing the other way round: import data from the Commons templates into relevant fields of Wikidata. Since the variety of ways a human can label or mislabel information in a template such as \"Category definition: Object\", I think that the script should be a helper tool to import data: it is to be ran on one category at a time, with a human checking the result, and correcting and completing the Wikidata entry as required. For now, I have been testing and refining my script over subcategories of [2] Category:Ship models in the Musée national de la Marine. You can see the result in the first 25 categories or so, and the corresponding Wikidata entries.The tool is presently in the form of a Python script with a simple command-line interface: ./read_commons_template.py Category:Scale_model_of_Corse-MnM_29_MG_78 reads the information from Commons, parses it, renders the various fields in the console for debugging purposes, and creates the required Wikibase objects (e.g: text field for inventory numbers, Q-Items for artists and collections, WbQuantity for dimensions, WbTime for dates, etc.) ./read_commons_template.py Category:Scale_model_of_Corse-MnM_29_MG_78 --commit does all of the above, creates a new Q-Item on Wikidata, and commits all the information in relevant fields. Ideally, when all the desired features will be implemented and tested, this script might be useful as a tool where one could enter the Licence of data to import (if relevant) The information is already on Wikimedia Commons and is common public knowledge. Discussion  Request process Two-years old request and not answered. I think you should ask for bot permission instead :) Vojtěch Dostál (talk) 12:47, 5 November 2020 (UTC) I think that this discussion is resolved and can be archived. If you disagree, don't hesitate to replace this template with your comment. Vojtěch Dostál (talk) 12:47, 5 November 2020 (UTC) Upload triples to Wikidata Request date: 1 November 2019, by: JLuzc Link to discussions justifying the request I discussed uploading triples to Wikidata that were extracted from Wikipedia, specifically from HTML tables. The final suggestion received from @ChristianKl: is in Topic:V9tqy53bfliigaly  Task description Uploading a list of 45K triples. The CSV file is available in Drive File, it contains [subject, predicate, object, wikipedia_source_page, precision_score] Licence of data to import (if relevant) Discussion  Request process @JLuzc: It's unclear what these data are.Vojtěch Dostál (talk) 12:51, 5 November 2020 (UTC)  I think that this discussion is resolved and can be archived. If you disagree, don't hesitate to replace this template with your comment. Vojtěch Dostál (talk) 12:51, 5 November 2020 (UTC) OpenCitations bot Request date: 2 June 2020, by: Csisc Link to discussions justifying the request https://etherpad.wikimedia.org/p/wikidata_covid_notes Task description The username of the concerned bot is OpenCitations Bot. This bot retrieves the Wikidata ID and DOI of scholarly publications using WDumper. Then, it uses the REST API of OpenCitations to retrieve the DOI of the references and citing works of each publication. Finally, the obtained DOI are converted to Wikidata ID using the WDumper-based dump and the final output is automatically added to Wikidata using QuickStatements API. The source code of this bot is build using Python 3.5. Licence of data to import (if relevant) The License of OpenCitations is CC0. Discussion @Csisc: I'd say go ahead and ask for bot permission, if you haven't already done that. Vojtěch Dostál (talk) 09:53, 5 November 2020 (UTC) Vojtěch Dostál: Please feel free to do this. Thank you. --Csisc (talk) 10:02, 5 November 2020 (UTC) Request process  I think that this discussion is resolved and can be archived. If you disagree, don't hesitate to replace this template with your comment. Vojtěch Dostál (talk) 09:53, 5 November 2020 (UTC) Magyarország közigazgatási helynévkönyve, 2018. január 1. (hungarian) Request date: 12 May 2019, by: Szajci Link to discussions justifying the request Sziasztok! Ezen a linken ([3]) elérhető a Magyarország közigazgatási helynévkönyve, 2018. január 1. című kiadvány. Van lehetőség arra, hogy a wikidatába beírja egy robot az adatokat? Kérlek titeket, írjon valaki valami biztatót Task description Licence of data to import (if relevant) Discussion @Szajci: How do you suggest to pair the identifier in the government database to the existing items? Vojtěch Dostál (talk) 12:48, 5 November 2020 (UTC) Request process  I think that this discussion is resolved and can be archived. If you disagree, don't hesitate to replace this template with your comment. Vojtěch Dostál (talk) 14:06, 9 November 2020 (UTC) \"Street in\" description Please change \"Street in \" to lowercase \"street in \" for [4] (currently >55000). I already fixed some, but there are just too many. --- Jura 12:59, 28 November 2020 (UTC) Done --Pasleim (talk) 17:57, 30 November 2020 (UTC) This section was archived on a request by: --- Jura 17:42, 30 November 2020 (UTC) Disambiguate \"Siracusa\" football clubs Request date: 5 April 2020, by: Matthew hk Link to discussions justifying the request Related discussions: [5] The item Q1788171 need to be deprecated, because, there is a need to disambiguation. There are actually 4 football team named after the city \" Siracusa\" (there are more if counting non-notable youth teams founded in 1996 as well as split the 2013 team due to yet another re-foundation in 2019 and yet another VAT number): Associazione Sportiva Siracusa (Q39052565) (?–1995) reason: folded in 1995 or 1996 U.S. Siracusa (Q28195113) (1996–2012) reason: renamed from \"U.S. Aldo Marcozzi 1956 \" in 1996. First team (not including youth section) folded in 2012, whole club went dormant in 2014 A.S.D. Città di Siracusa (2012) (Q89498813) (2012–2013) reason: newly established club that have no relation to the old club except also named after the city Siracusa Calcio 1924 (Q4647072) (2013–present) reason: renamed from \"A.C. Palazzolo A.S.D.\" and relocated from the city of the same name  Task description If the parameter contain Q1788171, disambiguation with above time frame. Usually the parameter found in player entry will have start time at that club and end time at that club. No action if no time provided . Licence of data to import (if relevant) Discussion @Matthew hk: Hello, how is this a bot request, please? Bots do edits on hundreds, thousands of items.Vojtěch Dostál (talk) 15:11, 9 November 2020 (UTC) fixed Vojtěch Dostál (talk) 15:13, 9 November 2020 (UTC) Request process This section was archived on a request by: --- Jura 07:52, 3 December 2020 (UTC) Mark deprecated P1255 if the value starts with vlts Request date: 22 November 2020, by: Eru Link to discussions justifying the request Property_talk:P1255#Old_viaf_componant_with_vtls https://fr.wikipedia.org/wiki/Discussion_utilisateur:FERNANDES_Gilbert#Module_Autorité Task description The vlts* value for HelveticArchives ID (P1255) should be marked as deprecated, the link is not working, it has be deleted on viaf.QuikStatement and OpenRefine cannot change the rank, so I think we need a bot to do that.Here is the SPARQL query tested on PetScan (319 results):SELECT ?human ?humanLabel ?BNS
WHERE
{
  ?human wdt:P31 wd:Q5.
  ?human wdt:P1255 ?BNS.
  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }
  FILTER(STRSTARTS(?BNS, 'vtls')) .
}
 Try it! – The preceding unsigned comment was added by Eru (talk • contribs). It seems they have been added erroneously with the property: they should be removed. --- Jura 11:15, 22 November 2020 (UTC) Licence of data to import (if relevant) Discussion Hi Jura1, I'm not sure, the value was good at the time, but it has been removed from viaf. Hsarrazin thinks it should be deprecated.If it is necessary to remove it, I just prepared a QS to do so. — eru [Talk] [french wiki] 12:42, 22 November 2020 (UTC) It seems that at some point someone repurposed the property [6] to include values that might have worked in some configuration, but now doesn't any more. The values don't seem to correspond to the actual property definition. If they fit somewhere else, one could move them there. @Manu1400: what do you think of it? --- Jura 13:15, 22 November 2020 (UTC) The delete batch is running: 1606572936706 — eru [Talk] [french wiki] 14:21, 28 November 2020 (UTC) Request process This section was archived on a request by: --- Jura 07:49, 3 December 2020 (UTC) ORCID for Wikidata 3 Request date: 27 July 2020, by Eva: EvaSeidlmayerRegarding the reasonable objection by Andrew (see my thread of 3.June 2020) I reviewed the project for matching authors and papers in Wikidata. Now, only already in Wikidata existing authors-items and already in Wikidata existing publications-items are matched.ORCID contains in 2019 eleven archive files. For the first archive file we had been able to detected: 457K Wikidata publication-items (3.8M publications in total)
 425K publication-items do not have any author-item registered
  32K publications are identified in Wikidata with registered authors
   of those 32K publication-items: 
             3.7K author-items listed in Wikidata are correct allocated to their publication-items (11.7%)
             4.2K author-items listed in Wikidata are not yet allocated to publication-items (24.6%)
             The other authors are not registered to Wikidata yet. These are the numbers only for the *first* of *eleven* ORCID-files. Would be cool to introduce the matching of authors to publications on ORCID basis.Please check the GitHub repo: https://github.com/EvaSeidlmayer/orcid-for-wikidataCheers, Eva Link to discussions justifying the request  Task description Licence of data to import (if relevant) The data provided by ORCID is licenced with CC0: https://orcid.org/about/what-is-orcid/principles Discussion \"see my thread of 3.June 2020\" - Please provide a link. Andy Mabbett (Pigsonthewing); Talk to Andy; Andy's edits 12:44, 14 August 2020 (UTC) @Pigsonthewing: Wikidata:Bot_requests#ORCID_for_Wikidata Vojtěch Dostál (talk) 09:56, 5 November 2020 (UTC) Thank you; in that case, @Andrew Gray: who is mentioned by first name only in the OP. Can we se a sample batch of (say) 100 edits? Also, note that ORCID's annual data dump for 2020 was made available last month. Andy Mabbett (Pigsonthewing); Talk to Andy; Andy's edits 10:49, 5 November 2020 (UTC) Request process Administrative division code of Chongqing Municipal District, People's Republic of China Request date: 8 August 2020, by: RedLightPOP Link to discussions justifying the request  Task description 开县, 梁平县, and 武隆县 were turned counties into districts, administrative division codes have also changed. China administrative division code in the data items is changed as follows: The beginning of the value needs to be changed from 50 02 34 to 50 01 54. The beginning of the value needs to be changed from 50 02 28 to 50 01 55. The beginning of the value needs to be changed from 50 02 32 to 50 01 56. Licence of data to import (if relevant) Discussion @RedLightPOP: The existing statements should not be changed. Instead, add a new statement with the newly-valid value and set it as BestRank. Vojtěch Dostál (talk) 12:43, 5 November 2020 (UTC) Request process cleanup of English descriptions for people Special:Search/\"born:\" \"died:\" haswbstatement:P31=Q5 finds some 10,000 items, many of which could use a cleanup of the description. --- Jura 14:33, 29 August 2019 (UTC) Would still be good to have. --- Jura 21:19, 26 March 2020 (UTC) @Jura1, Ham II :Should we fill the description with something like \"Person\" or just remove the description? --Kanashimi (talk) 08:55, 8 June 2020 (UTC) For descriptions like \"born 1874; born in ; died 1933; died in\" a replacement of \"(1874-1933)\" would do [7]. If the dates are present as statements, one could remove them entirely and add something else in another run. Please don't add \"person\" or \"human\" as English description. --- Jura 08:59, 8 June 2020 (UTC) It seems there are too many descriptions must be semantically analyzed to understand, and not easy to extract information. Sorry that I can not continue the task. :( --Kanashimi (talk) 11:39, 8 June 2020 (UTC) I don't think this can be easily automated by bot because descriptions vary a lot. Some sort of machine learning might be able to achieve that, but simple RegEx search and replace will only either find too few cases, or create a lot of errors if it's too broad. Also I'd oppose automatic removal of dates from descriptions - there is sometimes a good reason for them to be there, eg. for simple disambiguation. Vojtěch Dostál (talk) 09:24, 5 November 2020 (UTC) The search above gives much cleaner results (the request is from August 2019), but it's still needed. Searching with some offset finds still the more problematic ones [8]. The idea is not suppress these entirely, but to avoid raw data in the description. Also, I don't think full dates of birth are needed. Samples: https://www.wikidata.org/w/index.php?title=Q59529559&oldid=1278812811 https://www.wikidata.org/w/index.php?title=Q59527888&oldid=1280416359 I think they are fairly straightforward to fix. Sample: [9]. This especially as P569/P570 are already present. To clean this up, we should probably identify the source of these strings and ensure that people who generate them (or defined MxM catalogues) attempt to fix them. --- Jura 12:46, 6 November 2020 (UTC) Ah, yes, these strings. I think they are created by Magnus Manske's Reinheitsgebot from reconciled VIAF groups without Wikidata item.Vojtěch Dostál (talk) 14:00, 9 November 2020 (UTC) Some catalogues have cleaner output than others. I think it depends on who defined them and/or what data was available. --- Jura 14:09, 9 November 2020 (UTC) Translation of the Indian Women Entreprenuer in Gujarati Request date: 7 May 2020, by: Haisebhai Link to discussions justifying the request  https://www.wikidata.org/wiki/Wikidata:Dataset_Imports/Translation_of_the_Indian_Women_Entreprenuer_in_Gujarati#DATA Task description  Licence of data to import (if relevant) Discussion  Request process @Haisebhai: What do you want us to do with these data? Import them as labels to the items? This can be done with QuickStatements tool.Vojtěch Dostál (talk) 14:11, 9 November 2020 (UTC) Complete Google Books ID (P675) from Internet Archive ID (P724) Many books in the Internet Archive come from Google Books (generally indicated as source). Accordingly, we could probably complete Google Books ID (P675) from Internet Archive ID (P724).Maybe the inverse could be done too (find IA based on GB). --- Jura 11:25, 18 July 2020 (UTC) @Jura1: Can you give an example of a book item which could be completed like that? Vojtěch Dostál (talk) 09:41, 5 November 2020 (UTC) Properties used: Internet Archive ID (P724)   , Google Books ID (P675)   SELECT ?item ?itemLabel ?value
{
  ?item wdt:P724 ?value .
  FILTER ( contains(?value, \"goog\") )
  FILTER NOT EXISTS { ?item wdt:P675 [] }
  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\"  }
}
LIMIT 100 Try it!It probably works for all the above. Probably many more. --- Jura 10:26, 5 November 2020 (UTC) @Jura1: Like this? Vojtěch Dostál (talk) 13:04, 5 November 2020 (UTC) Looks good. I think had just added P724 as ref, but I suppose your approach is the more complete one. BTW I'm not entirely sure how to identify the ones without \"goog\" in the id. --- Jura 12:55, 6 November 2020 (UTC) @Jura1: OK. I'll import those. Should I ignore the constraint warning such as the one in Cur Deus homo (Q83522)? Or should I skip it if not instance of/subclass of version, edition or translation (Q3331189)? Vojtěch Dostál (talk) 19:20, 6 November 2020 (UTC) It seems the constraints between the two differ despite the fact that both identify the same. Personally, I think it helps determine if something should be done with the IA identifier. Will you test all 217324 IA identifiers without P675 or just the 407 goog ones? --- Jura 19:42, 6 November 2020 (UTC) Hmm, OK. I can try it with all but the scraping will take ages, I'll tell you when it's done. Vojtěch Dostál (talk) 19:21, 8 November 2020 (UTC) Upgrade URLs to HTTPS, whenever possible Request date: 4 May 2020, by: OsamaK Task description With over 85% of all web requests encrypted via HTTPS, the new normal has shifted to expecting web links to lead to an HTTPS version whenever a website supports it. There are two credible sources that maintain lists of supported websites: Chrome's HSTS Preload List (used by hundred of millions of Chrome/Chromium users) and EFF's HTTPSEverywhere's atlas (used by 2 million+ Chrome users and ~1 million Firefox users). Applying thes lists would lead to the exact same versions of any given web page, but via HTTPS whenever possible. I would like to create a bot that upgrades the HTTP URLs of the following properties to HTTPS: official website (P856) official blog URL (P1581) web feed URL (P1019) described at URL (P973) --OsamaK (talk) 18:24, 4 May 2020 (UTC) Discussion Support I asked @Laurentius: to do something similar on Italian Wikipedia and it worked very well, maybe his bot is available also for Wikidata ;-) --Epìdosis 22:28, 14 June 2020 (UTC) Yes, I did exactly the same job on Italian Wikipedia (using the same sources of data: Chrome's HSTS preload list and HTTPS Everywhere). I could easily adapt my bot to Wikidata (it is actually easier than working on Wikipedia); I'd be happy to do it, and in any case Support. - Laurentius (talk) 18:19, 28 June 2020 (UTC) That's great! Thank you, Epìdosis for bringing up this experience. At this stage, it's clear that enough time has passed for this bot approval, and since I don't yet have working code, could you, Laurentius, take this bot task on and start testing and documenting the initial tests here?--OsamaK (talk) 13:31, 3 July 2020 (UTC) Support --Haansn08 (talk) 12:16, 30 August 2020 (UTC) Support and @Laurentius: Hello Laurentius, are you planning to work on this? I am trying to clean up this page a bit :) Vojtěch Dostál (talk) 15:58, 9 November 2020 (UTC) Request process Getting number of cases (P1603) to rank most current figure as preferred Request process Request date: 6 June 2020, by: Sdkb Link to discussions justifying the request Wikidata:Project chat#Getting number of cases (P1603) to rank most current figure as preferred Task description Following up from here, could number of cases (P1603), number of deaths (P1120), and number of recoveries (P8010) be made so that they automatically mark the most recent value as preferred? Licence of data to import (if relevant) Discussion  Request process @Sdkb: This is a broader issue, isn't it? Should more properties work like this? Maybe this should be discussed at Project Chat again.Vojtěch Dostál (talk) 14:13, 9 November 2020 (UTC) There was some discussion of this also at Wikidata_talk:WikiProject_COVID-19/Data_models/Outbreaks#Should_we_skip_fiddling_with_preferred_rank_until_the_outbreak_is_over?. --- Jura 15:11, 9 November 2020 (UTC) Permissions for new qualifiers \"latest start date\" and \"earliest end date\" The new qualifiers latest start date (P8555) and earliest end date (P8554) should be valid anywhere start time (P580) and end time (P582) are valid. Can someone set a Bot to find the appropriate items and add these as valid qualifiers?@Jheald, ArthurPSmith, Gamaliel, Sic19: FYI! - PKM (talk) 23:04, 18 September 2020 (UTC) @PKM: Here's a query for the properties to update: https://w.wiki/cit Not sure whether QuickStatements can do this. We should probably make sure that earliest date (P1319) and latest date (P1326) are both also permitted. Jheald (talk) 08:58, 19 September 2020 (UTC) +1 - PKM (talk) 18:51, 19 September 2020 (UTC) @PKM: You can use this Petscan query to do that. Vojtěch Dostál (talk) 09:34, 5 November 2020 (UTC) @Vojtěch Dostál: Thank you! I should be able to figure that out. - PKM (talk) 05:59, 6 November 2020 (UTC) @PKM: Can you double check your edits? I don't think many additions are of much use, e.g. at TikTok username (P7085), Commons quality assessment (P6731). In the future, please avoid making large scale changes to property constraints without prior discussion on project chat. --- Jura 05:47, 8 November 2020 (UTC) Lucas Werkmeister (WMDE) Jarekt - mostly interested in properties related to Commons MisterSynergy John Samuel Sannita Yair rand Jon Harald Søby Pasleim Jura PKM ChristianKl Sjoerddebruin Fralambert Manu1400 Was a bee Malore Ivanhercaz Peter F. Patel-Schneider Pizza1016 Ogoorcs ZI Jony Eihel cdo256 Epìdosis Dhx1 99of9 Mathieu Kappler Lectrician1 SM5POR Infrastruktur SamoasambiaNotified participants of WikiProject property constraints --- Jura 05:48, 8 November 2020 (UTC)It doesn't seem very problematic to me. Vojtěch Dostál (talk) 14:02, 9 November 2020 (UTC) It's a problem when people start using them instead of the ones that are expected. --- Jura 14:23, 9 November 2020 (UTC) But the rationale here is that latest start date (P8555) and earliest end date (P8554) are expected wherever start time (P580) and end time (P582) are expected... Vojtěch Dostál (talk) 16:03, 9 November 2020 (UTC) Actually, one could use it always instead of the others. --- Jura 13:13, 11 November 2020 (UTC) Research institutes in France Items used: France (Q142)   Properties used: country (P17)   , located in the administrative territorial entity (P131)   , street address (P6375)   """@en;
  dcterms:isPartOf <https://www.wikidata.org//wiki/Wikidata:Bot_requests/Archive/2020/11>;
  dcterms:license <https://creativecommons.org/licenses/by-sa/4.0/>;
  sh:prefixes _:wikidata_prefixes;
  schema:target <https://query.wikidata.org/sparql/> .
